\chapter{Overview}
One of the most popular observations in computer science is Moore's Law.
It was introduced in the mid-1960s when Gordon E. Moore described an empirical relationship \cite{Moore1965}
projecting that the number of transistors per IC (Integrated Circuit) will increase twofold every year.
He predicted that this trend will continue for at least a decade, but remarkably enough this is still true 50 years later. 
For most of that time CPU (Central Processing Unit) consisted of only one single processing core with performance growing through a combined increase of clock frequency speed and number of components per IC. At some point though, CPUs ran into a bottleneck,
further clock frequency increases would come with drastically higher heat production and energy consumption \cite{Illinois}. Thus it became clear that to develop CPUs which will meet ever increasing demand for computing power, new architectural designs have to be engineered.

Before multi-core CPUs became commercialy available, Intel came up with a new idea of boosting performance, SMT (Simultaneous Multi-Threading) and it's implementation - Hyper-Threading. It was first included with Pentium 4 and Xeon processors in 2002. Architecturally, a processor with Hyper-Threading Technology consists of two logical processors per core, each of which has its own processor architectural state. Each logical processor can be individually halted, interrupted or directed to execute a specified thread, independently from the other logical processor sharing the same physical core.
Unlike a traditional dual-processor configuration that uses two separate physical processors, the logical processors in a hyper-threaded core share the execution resources. Intel claimed that they would get a performance boost around 15 to 30\%~\cite{intel} compared to other non-Hyper-
Threaded CPUs and only increase the size of the die with 5\%. AMD in response developed the Bulldozer architecture, released in 2011, based on CMT (Cluster Multi-Threading), but it never delivered satisfactory performance increases, thus it was replaced by the SMT Zen architecture in 2017, which will be described later.

Soon after, in 2005, world's first dual core processors were introduced, first by AMD (Athlon 64 x2) then by Intel (Pentium D). Cores in such processors are separate physical entities and can execute instructions in a truly simultaneous way. In specific architectures cores may or may not share caches, and they may implement message passing or shared-memory inter-core communication methods. 

With the advent of multi-core processors came a complete programming paradigm shift. Performance gains, while substantial, require programmers to divide their programs into concurrent parts and synchronize access to data shared between threads. To fully reap the rewards of the new paradigm programs need to be structured in way which parallelizes as much of it as possible while all necessary sequential fraction of the program are kept to a minimum (Amdahl's law~\cite{Amdahl1967}).

.NET Framework, software framework developed by Microsoft, embraced multi-threading from it's very beginning in 2001.
In the earliest versions tools available in C\#, multi-paradigm programming language,  were quite basic, low-level primitives that dealt with monitoring and synchronization of threads. In 2005 F\# was added to .NET family, functional language based on ML (MetaLanguage). As it will be shown, functional paradigm had and still has great influence in the development of parallel programming in .NET. Major advancements came with 2009's .NET Framework 4.0 release.TPL (Task Parallel Library), PLINQ (Parallel Language Integrated Query), concurrent collections and others were introduced in this update. 


\section{Goals and scope}

The goal for this thesis is to look into various sequential software pieces
and explore the potential performance gains by using using concurrency tools 
provided by the .NET platform. Known sequential algorithms and existing business software will be used.

Parallel versions will be implemented using diversified set of tools available on .NET platform, namely
Task Parallel Library PLINQ, immutable and concurrent collections, with code pieces written both
in C\#.

Extensive tests on runtime performance and memory allocation will be performed. 
These will help to draw meaningful conclusions about parallel programming paradigm, set of precautions
and guidelines which will help in future software development endevours. 

\section{Thesis structure}
\textbf{Chapter 2 - Concurrency and Threads in .NET} -  background information on concurrency in general. 
Broad description of terms, classes, interfaces and tools commonly used in concurrent implementions on the .NET platfom.

\noindent
\textbf{Chapter 3 - Test Environment} - overview of testing environment. Specification of hardware and software benchmarking tools used in experiments.

\noindent
\textbf{Chapter 4 - Algorithms and software} - explanation of algorithms and software selected as the test subjects in the thesis. Description of sequential versions of the software, explanation on how they work and implementation of their parallel counterpart. 

\noindent
\textbf{Chapter 5 - Experiments} - results of testing run time performance and memory allocation of sequential and parallel implementations of software pieces.

\noindent
\textbf{Chapter 6 - Discussion} - in depth analysis of experiments and results, theoretical viewpoints on concurrent programming.

\noindent
\textbf{Chapter 7 - Conclusions} - summary of results, advice for developing parallel algorithms, consideration for future research.

