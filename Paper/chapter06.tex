\chapter{Discussion}
After carefully designing the experiments in chapter~\ref{chap:3} and \ref{chap:4} and conducting them in chapter\ref{chap:5} this part will discuss the outcomes of the benchmarks. 
After interpreting and analyzing the results, a set of guidelines for future .NET parallel ventures will be presented. \\
 
At the stage of development of algorithms and software, it quickly became clear that benchmarking is crucial when programming for performance. 
Sometimes a small tweak in an unimportant subroutine would completely change the speed of execution, thus many iterations of the code were discarded even before the experiments proper. 
Some of these haphazard implementations were included in QuickSort part (section~\ref{sec: QuickSort}). Async-await state machine is a useful construct, but it was build for fluidity of applications, not for performance. Without benchmarking, such code might have slipped into production environment for a potential disaster. \emph{BenchmarkDotNet} enables programmers
to scale-back the tests, so they can be used during development or even use them as part of their unit test suite.

Platform version is very significant when it comes to performance. .NET, thanks to being open source, is often updated with small fixes and improvements which aren't always included in the major release notes. Such updates may have big impact on the application, especially if it uses high-level constructs like LINQ. This was confirmed by tests conducted in QuickSort part (section~\ref{sec: QuickSort}). Switching from .NET Framework 4.7 to .NET Core 3.1 brought major improvements across all versions of the algorithm. Further, test were conducted only with .NET Core 3.1 since the first example was enough to drive the point.

Another point that came out of QuickSort part (section~\ref{sec: QuickSortImp}) is that specific implementation matters. The imperative version of the algorithm highly overperformed the functional one and didn't benefit as much from parallelisation. The cost in this case came from other factors: length of the implementation, understandability and reliability. Imperative version took many iterations to get just right, even with something relatively simple as the QuickSort algorithm. The end result is a convoluted, hard to read and understand piece of software. On the other hand, version using LINQ was simple to write and end result is comprehensive and concise. Any further maintenance over this version would be significantly easier. One has to consider cost-benefit of programming for performance when choosing the specific implementation. 

When using parallelism, it is important to gauge the amount of tasks spawned by the software. Otherwise, resource heavy context switches will overhelm any potential speed benefits of parallel programming. This was demonstrated in QuickSort algorithm (section~\ref{sec: QuickSort}) and Mandelbrot set drawing (section~\ref{sec: Mandelbrot}). In the former, lack of depth control together with async-await rendered the implementation completely inefficient. In the latter, using two nested \texttt{Parallel.For} loops performed worse than using only one. We can hypothesize that further parallelisation would decrease the performance even more. 

Data parallelism Fork/Join and MapReduce patterns proved to be useful and powerful. The former was used in the implementation of K-Means algorithm (section~\ref{sec: KMeansImp}) and the latter in software ranking NuGet packages (section~\ref{sec: NuGetImp}). Both of these patterns can be implemented in a generic way, allowing programmers to drop them in when they see fit. Parallelisation was again streamlined thanks to the use of LINQ, turning the queries into parallel ones required few PLINQ lines. This makes the code clear and easy to understand, on contrary to often convoluted parallel implementations. 

Load balancing with a data partitioner is a strategy worth considering when implementing parallel queries. This addition was especially performant with larger datasets in NuGet package ranking software (section~\ref{sec: NuGet}), not so much with smaller datasets like in the case of Madelbrot algorithm (section~\ref{sec: Mandelbrot}). 

Almost all parallel implementations came with a cost of memory consumption oscillating around 15\%. Considering that memory is abundant in modern computing machines, this shouldn't be an important factor unless working on a device which has for some reasons significant memory constraints. Special case can be made for algorithm which allocate a lot of memory by object creation, as in the case of Mandelbrot algorithm (section~\ref{sec: MandelbrotImp}) which creates millions of objects inside its functions. When Garbage Collector cleanups become the bottleneck and stop the program from executing, it might be worth to consider using value types structures instead (section~\ref{sec: Mandelbrot}).\\

To summarize this analysis: 
\begin{itemize}
	\item Benchmarking is crucial at all stages of development.
	\item \texttt{BenchmarkDotNet} is recommended as the go-to library for .NET benchmarking.
	\item Modern platforms are optimized for performance, working on newest versions should be a~priority.
	\item Parallel programs should be written with understandability, reliability and maintainability in mind. Functional paradigm offers great tools which enable this goal.
	\item Depth of parallelisation should always be controlled for, exact numbers should be data driven.
	\item Fork/Join and MapReduce patterns are recommended in data parallelism problems. 
	\item Data load balancing with partitioners is recommended for large datasets.
	\item Do not preemptively optimize for memory consumption unless it is required by specific scenario.
	\item If scenario includes creating many objects inside local function scope, consider using value type instead of reference type.

\end{itemize}
