\chapter{Algorithms and software}
\section{Quicksort}
Quicksort is an in-place sorting algorithm, developed by British computer scientist Tony Hoare in 1959. It is a divide-and-conquer algorithm. It works by selecting a 'pivot' and dividing a large array into two sub-arrays of low and high elements, depending if they are bigger or smaller than the pivot. Recursive algorithms, especially ones based on a form of divide-and-conquer , are a great candidate for parallelization and CPU-bound computations. Quicksort pseudocode (shown in Lst. \ref{lst:PseudoQuickSort}) is based on description from \emph{Introduction to Algorithms} \cite{Cormen2009}.

\begin{lstlisting}[basicstyle=\ttfamily, caption={Sequential Quicksort pseudocode}, label={lst:PseudoQuickSort}]

QUICKSORT(A, p, r)
	if p < r
	q = PARTITION(A, p, r)
	QUICKSORT(A, p, q - 1)
  QUICKSORT(A, q + 1, r)
	
PARTITION(A, p, r)
	x = A[r]
	i = p - 1
  for j = p to r - 1
		if A[j] <= x
			i = i + 1
			exchange A[i] with A[j]
	exchange A[i + 1] with A[r]
	return i + 1
\end{lstlisting}

\subsubsection{Imperative sequential implementation}
This version is a "traditional" implementation of the algorithm. It is written in imperative style and quite lengthy (Lst. \ref{lst:ImpSeqQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Imperative sequential quicksort implementation}, label={lst:ImpSeqQuickSort}]
private static void Sort(int[] source, int left, int right)
{
  if (left >= right) return;
  var pivot = Partition(source, left, right);
  Sort(source, left, pivot);
  Sort(source, pivot + 1, right);
}

private static int Partition(int[] source, int left, int right)
{
  var pivot = (right + left) / 2;
  var pivotInt = source[pivot];

  Swap(ref source[right - 1], ref source[pivot]);
  var store = left;
  for (var i = left; i < right - 1; ++i)
  {
    if (source[i].CompareTo(pivotInt) >= 0) continue;
    Swap(ref source[i], ref source[store]);
    ++store;
  }

  Swap(ref source[right - 1], ref source[store]);
  return store;
}

private static void Swap(ref int a, ref int b)
{
  var temp = a;
  a = b;
  b = temp;
}
\end{lstlisting}

\subsubsection{Imperative parallel implementation}
Parallel version was implemented in a manner in which somebody who is not well versed in .NET's parallelism would implement it. Many developers know async-await state machine because is often used in projects with asynchronous operations . On top of that no precautions for thread oversaturation were taken (Lst. \ref{lst:ImpParQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Imperative parallel quicksort}, label={lst:ImpParQuickSort}]
private static async Task Sort(int[] source, int left, int right)
{
  if (left >= right) return;

  var pivot = Partition(source, left, right);

  await Task.Run(() => Sort(source, left, pivot));
  await Task.Run(() => Sort(source, pivot + 1, right));
}
\end{lstlisting}

\subsubsection{Imperative optimized implementation}
Optimized version uses depth control and removes the use of async-await state machine.
(Lst. \ref{lst:ImpOptQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Imperative optimized quicksort}, label={lst:ImpOptQuickSort}]
private static void Sort(int[] source, int left, int right, int depth)
{
  if (left >= right) return;

  var pivot = Partition(source, left, right);
  if (depth > 0)
  {
    Parallel.Invoke(
      () => Sort(source, left, pivot, depth--),
      () => Sort(source, pivot + 1, right, depth--));
  }

  else
  {
    Sort(source, left, pivot, depth);
    Sort(source, pivot + 1, right, depth);
  }
}
\end{lstlisting}

\subsubsection{Functional sequential implementation}
This version of the algorithm was implemented using LINQ in a functional, declarative manner. It is conspicuous that this version is more concise and easier to read. (Lst. \ref{lst:FunSeqQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Functional sequential quicksort implementation}, label={lst:FunSeqQuickSort}]
private static ImmutableList<int> Sort(ImmutableList<int> source)
{
  if (!source.Any()) return source;

  var first = source.First();
  var (left, right) = source
    .Skip(1)
    .Partition(x => x <= first);
  var leftSorted = Sort(left);
  var rightSorted = Sort(right);

  return leftSorted
    .Add(first)
    .AddRange(rightSorted);
}
\end{lstlisting}

\subsubsection{Functional parallel implementation}
Parallel implementation was achieved in the same way, using \emph{Task.Run} and async-await state machine. 
(Lst. \ref{lst:FunParQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Functional parallel quicksort}, label={lst:FunParQuickSort}]
private async Task<ImmutableList<int>> Sort(ImmutableList<int> source)
{
  if (source.None()) return source;

  var first = source.First();
  
  var (left, right) = source
    .Skip(1)
    .Partition(x => x <= first);
  var leftSorted = await Task.Run(() => Sort(left));
  var rightSorted = await Task.Run(() => Sort(right));

  return leftSorted
    .Add(first)
    .AddRange(rightSorted);
}
\end{lstlisting}

\subsubsection{Functional optimized implementation}
Optimized version uses depth control and removes the use of async-await state machine.
(Lst. \ref{lst:FunOptQuickSort}).
\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Functional optimized quicksort}, label={lst:FunOptQuickSort}]
private static Task<ImmutableList<int>> Sort(ImmutableList<int> source, int depth)
{
  if (source.None()) return Task.FromResult(source);

  var first = source.First();
  var (left, right) = source
    .Skip(1)
    .Partition(x => x <= first);
  var leftSorted = ParallelizeUnlessDepthIsReached(x => Sort(left, x), depth);
  var rightSorted = ParallelizeUnlessDepthIsReached(x => Sort(right, x), depth);

  return Task.FromResult(leftSorted.Result
    .Add(first)
    .AddRange(rightSorted.Result));
}

private static Task<T> ParallelizeUnlessDepthIsReached<T>(Func<int, Task<T>> func, int depth)
{
  return depth > 0 ? Task.Run(() => func(depth - 1)) : func(0);
}
\end{lstlisting}



\clearpage
\section{K-means clustering}
k-means, otherwise commonly know as Lloyd's algorithm, is an unsupervised machine learning algorithm which is used to process a data set into clusters.
They represent a geometric shape with a mass center, a centroid.
Each of the clusters has its own centroid which is the sum of points divided by number of total points. 

The algorithm is an iterative process which repeats until it reaches a convergence point or exceeds the limit (sometimes it does not converge).
Each of the iterations updates the centroids to produce better clusters.
The iteration involves these steps:
\begin{enumerate}
	\item Sum up points in each cluster.
	\item Divide each sum by the number of points in respective cluster.
	\item Reassign all points to a closest centroid, distance is calculated through Euclidean distance function.
	\item Repeat until locations stabilize.
\end{enumerate}

In the following subsections 3 implementations of centroid recalculation algorithm will be presented. Each of the them is injected into k-means algorithm (Lst. \ref{lst:KMeans}) during performance benchmarking.

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={K-means centroid computation}, label={lst:KMeans}]
//For clarity
using DataSet = System.Collections.Immutable.ImmutableList<System.Collections.Immutable.ImmutableList<double>>;

public static DataSet ComputeCentroids(Func<DataSet, DataSet> updateCentroids, DataSet centroids,
  int rowLength)
{
  for (var i = 0; i <= 1000; i++)
  {
    var (newCentroids, error) = GetNewCentroidsAndError(updateCentroids, rowLength, centroids);

    if (error < 1e-9)
      return newCentroids;
    
    centroids = newCentroids;
  }

  return centroids;
}

private static (DataSet, double) GetNewCentroidsAndError(Func<DataSet, DataSet> updateCentroids, int rowLength, DataSet centroids)
{
  var newCentroids = updateCentroids(centroids).SortCentroids(rowLength);
  var error = double.MaxValue;

  if (centroids.Count == newCentroids.Count)
    error = centroids.Select((t, j) => DistanceTo(t, newCentroids[j])).Sum();

  return (newCentroids, error);
}


private static DataSet SortCentroids(this DataSet result, int RowLength)
  => result.Sort((a, b) =>
  {
    for (var i = 0; i < RowLength; i++)
      if (a[i] != b[i])
        return a[i].CompareTo(b[i]);
    return 0;
  });


public static ImmutableList<double> GetNearestCentroid(DataSet centroids, ImmutableList<double> center) 
  => centroids.Aggregate((centroid1, centroid2) => 
    centroid2.DistanceTo(center) < centroid1.DistanceTo(center)
      ? centroid2
      : centroid1);

private static double DistanceTo(this ImmutableList<double> @from, ImmutableList<double> to)
{
  var results = 0.0;
  for (var i = 0; i < @from.Count; i++)
    results += Math.Pow(@from[i] - to[i], 2.0);
  return results;

}
\end{lstlisting}

\subsubsection{Sequential implementation}

\emph{UpdateCentroids} evaluates each cluster and reassigns centroids to newly calculated centers in two steps. In the first one \emph{GroupBy} function is used to aggregate the points using the \emph{GetNearestCentroid} function as a key. In the next one, each point grouping is used to calculate new centers for each given point. This step is achieved via local accumulator in \emph{Aggregate} function and collection combining in \emph{Zip} function which threads centroids and accumulator sequences. Finally, each center is determined through dividing with number of points in the cluster (Lst. \ref{lst:SeqKMeans}).


\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Sequential k-means algorithm}, label={lst:SeqKMeans}]
private Func<DataSet, DataSet> UpdateCentroids()
  => centroids =>
    Source
      .GroupBy(row => KMeans.GetNearestCentroid(centroids, row))
      .Select(CalculateCenter)
      .ToImmutableList();

private ImmutableList<double> CalculateCenter(IGrouping<ImmutableList<double>, ImmutableList<double>> points) 
  => points
    .Aggregate(new double[RowLength], (acc, item) => acc.Zip(item, (a, b) => a + b).ToArray())
    .Select(items => items / points.Count())
    .ToImmutableList();
\end{lstlisting}

\subsubsection{Parallel implementation}

This  implementation uses PLINQ (TODO: Add reference) which easily transforms sequential LINQ query into a parallel one with 2 lines of code.
This was enabled by using \emph{Aggregate}, LINQ's equivalent of functional sequence transforming function: \emph{Seq.Fold}. Force parallelism execution mode is used to make sure that PLINQ internal mechanism won't transform the query into a sequential one.

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel k-means algorithm}, label={lst:ParKMeans}]
private Func<DataSet, DataSet> UpdateCentroids()
  => centroids =>
    Source
      .AsParallel()
      .WithExecutionMode(ParallelExecutionMode.ForceParallelism)
      .GroupBy(u => KMeans.GetNearestCentroid(centroids, u))
      .Select(CalculateCenter)
      .ToImmutableList();

private ImmutableList<double> CalculateCenter(IGrouping<ImmutableList<double>, ImmutableList<double>> points)
  => points
    .Aggregate(new double[RowLength], (acc, item) => acc.Zip(item, (a, b) => a + b).ToArray())
    .Select(items => items / points.Count())
    .ToImmutableList();
\end{lstlisting}

\subsubsection{Parallel implementation with partitioner}

To further boost the parallel implementation, data partitioner was used to load balance the data between threads (TODO: Add reference) (Lst. \ref{lst:PartParKMeans}). 

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel k-means algorithm with partitioner}, label={lst:PartParKMeans}]
private Func<DataSet, DataSet> UpdateCentroids()
  => centroids 
    => Partitioner
    .Create(Source, loadalBalance: true)
    .AsParallel()
    .WithExecutionMode(ParallelExecutionMode.ForceParallelism)
    .GroupBy(u => KMeans.GetNearestCentroid(centroids, u))
    .Select(CalculateCenter)
    .ToImmutableList();

private ImmutableList<double> CalculateCenter(IGrouping<ImmutableList<double>, ImmutableList<double>> points) 
  => points
    .Aggregate(new double[RowLength], (acc, item) => acc.Zip(item, (a, b) => a + b).ToArray())
    .Select(items => items / points.Count())
    .ToImmutableList();
\end{lstlisting}

\clearpage
\section{Mandelbrot}

Mandelbrot set images are made by sampling complex numbers and testing for each sample point whether the orbit of the critical point z = 0 under iteration of the quadratic map remains bounded (eq.\ref{eq:Mandelbrot}) \cite{MandelbrotExplorer}. Images of the Mandelbrot set display an elaborate boundary that reveals complex structure arising from the application of simple rules. It's one of the best-known examples of mathematical visualization. It was named in the tribute of a pioneer of fractal geometry, Benoit Mandelbrot, by Adrien Douady. \cite{Douady}

\begin{equation}
\centering 
z_n + 1 = z_n^2 + c
\label{eq:Mandelbrot}
\end{equation}

In the following subsections 3 implementations of Mandelbrot set drawing algorithms will be presented. Each of the them is injected into bitmap generating code (Lst. \ref{lst:Bitmap}) during performance benchmarking. Example visualization made during the experiments is presented on Fig. \ref{fig:MandelbrotVis}.

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Mandelbrot bitmap generation}, label={lst:Bitmap}]
public static Bitmap DrawMandelbrotBitmap(Action<BitmapData, byte[]> mandelbrotDrawingStrategy, int rows, int columns)
{
  var bitmap = new Bitmap(rows, columns, PixelFormat.Format24bppRgb);
  var bitmapData = LockBitmap(bitmap);
  var allPixels = new byte[bitmapData.Stride * bitmap.Height];
  var firstPixel = bitmapData.Scan0;
  Marshal.Copy(firstPixel, allPixels, 0, allPixels.Length);

  mandelbrotDrawingStrategy(bitmapData, allPixels);

  Marshal.Copy(allPixels, 0, firstPixel, allPixels.Length);
  UnlockBitmap(bitmap, bitmapData);

  return (Bitmap)bitmap.Clone();
}

private static BitmapData LockBitmap(Bitmap bitmap) 
  => bitmap.LockBits(new Rectangle(0,
      0,
      bitmap.Width,
      bitmap.Height),
    ImageLockMode.ReadWrite,
    PixelFormat.Format24bppRgb);

private static void UnlockBitmap(Bitmap bitmap, BitmapData bitmapData) 
  => bitmap.UnlockBits(bitmapData);

\end{lstlisting}

\begin{figure}[!ht]
	\centering
		\includegraphics[width = 0.5\textwidth]{figures04/MandelbrotVis.png}
	\caption{Mandelbrot set visualization}
	\label{fig:MandelbrotVis}
\end{figure}

\pagebreak
\subsubsection{Sequential implementation}
This implementation consists of two nested loops. The outer one iterates over the columns of the bitmap while the inner one iterates over its rows. 
Pixel coordinates are translated into real and imaginary parts of a complex number by using the \emph{ComputeColumn} and \emph{ComputeRow} functions. Afterwards \emph{BelongsToMandelbrot} function checks if the number is part of the Mandelbrot set (Lst. \ref{lst:SeqMandelbrot}).

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Sequential Mandelbrot algorithm}, label={lst:SeqMandelbrot}]
private Action<BitmapData, byte[]> DrawingStrategy()
  => (bitmapData, pixels) =>
  {
    for (var column = 0; column < Columns; column++) 
    {
      for (var row = 0; row < Rows; row++) 
      {
        var x = Center.ComputeRow(row, Width, Columns); 
        var y = Center.ComputeColumn(column, Height, Rows); 
        var c = new ComplexNumber(x, y);
        var color = BelongsToMandelbrot(c, 100) ? Color.Black : Color.White; 
        var offset = (column * bitmapData.Stride) + (3 * row);
        pixels.WriteColors(offset, color);
      }
    }
  };

private static bool BelongsToMandelbrot(ComplexNumber number, int iterations)
{
  var zNumber = new ComplexNumber(0.0f, 0.0f);
  var accumulator = 0;
  while (accumulator < iterations && zNumber.Magnitude() < 2.0)
  {
    zNumber = zNumber.MultiplyWith(zNumber).AddTo(number);
    ++accumulator;
  }

  return accumulator == iterations;
}
\end{lstlisting}

\subsubsection{Parallel implementation}
Parallel version was implemented using Fork/Join (TODO: Add reference) pattern. TPL (TODO: Add Reference) enables us to easily apply this pattern with multiple parallelization constructs, in this case the \emph{Parallel.For} replacement for sequential loops. (Lst. \ref{lst:ParMandelbrot})

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel Mandelbrot algorithm}, label={lst:ParMandelbrot}]
private Action<BitmapData, byte[]> DrawingStrategy()
  => (bitmapData, pixels) =>
    Parallel.For(0, Columns - 1, column =>
    {
      for (var row = 0; row < Rows; row++)
      {
        var x = Center.ComputeRow(row, Width, Columns);
        var y = Center.ComputeColumn(column, Height, Rows);
        var c = new ComplexNumber(x, y);
        var color = BelongsToMandelbrot(c, 100) ? Color.Black : Color.White;
        var offset = (column * bitmapData.Stride) + (3 * row);
        pixels.WriteColors(offset, color);
      }
    });

private static bool BelongsToMandelbrot(ComplexNumber number, int iterations)
{
  var zNumber = new ComplexNumber(0.0f, 0.0f);
  var accumulator = 0;
  while (accumulator < iterations && zNumber.Magnitude() < 2.0)
  {
    zNumber = zNumber.MultiplyWith(zNumber).AddTo(number);
    ++accumulator;
  }
  return accumulator == iterations;
}
\end{lstlisting}

\subsubsection{Double parallel implementation}
This version replaced both sequential loops with TPL's \emph{Parallel.For} construct. It will be useful to examine how oversaturation impacts the algorithm performance (Lst. \ref{lst:DoubleParMandelbrot})

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Double parallel Mandelbrot algorithm}, label={lst:DoubleParMandelbrot}]
private Action<BitmapData, byte[]> DrawingStrategy()
  => (bitmapData, pixels) =>
    Parallel.For(0, Columns - 1, column =>
    {
      Parallel.For(0, Rows - 1, row =>
      {
        var x = Center.ComputeRow(row, Width, Columns);
        var y = Center.ComputeColumn(column, Height, Rows);
        var c = new ComplexNumber(x, y);
        var color = BelongsToMandelbrot(c, 100) ? Color.Black : Color.White;
        var offset = (column * bitmapData.Stride) + (3 * row);
        pixels.WriteColors(offset, color);
      });
    });

private static bool BelongsToMandelbrot(ComplexNumber number, int iterations)
{
  var zNumber = new ComplexNumber(0.0f, 0.0f);
  var accumulator = 0;
  while (accumulator < iterations && zNumber.Magnitude() < 2.0)
  {
    zNumber = zNumber.MultiplyWith(zNumber).AddTo(number);
    ++accumulator;
  }
  return accumulator == iterations;
}
\end{lstlisting}

\subsubsection{Parallel implementation using value types}

In some cases .NET's Garbage Collector can be the bottleneck of the application. Reference type objects are allocated on the heap and are very cheap to use as a function argument since only the pointer is copied. These objects though have a memory overhead and may heavily tax the GC which will stop the execution of the program until cleanup is done. Contrary to that, value type objects are allocated on the stack and will never cause GC to pause the program. This version uses \emph{ComplexNumberStruct} objects which are identical to previous versions except they are value type instead of reference type.  (Lst. \ref{lst:ParStructMandelbrot})

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel Mandelbrot algorithm using value types}, label={lst:ParStructMandelbrot}]

private Action<BitmapData, byte[]> DrawingStrategy()
  => (bitmapData, pixels) =>
    Parallel.For(0, Columns - 1, column =>
    {
      for (var row = 0; row < Rows; row++)
      {
        var x = Center.ComputeRow(row, Width, Columns);
        var y = Center.ComputeColumn(column, Height, Rows);
        var c = new ComplexNumberStruct(x, y);
        var color = BelongsToMandelbrot(c, 100) ? Color.Black : Color.White;
        var offset = (column * bitmapData.Stride) + (3 * row);
        pixels.WriteColors(offset, color);
      }
    });


private static bool BelongsToMandelbrot(ComplexNumberStruct number, int iterations)
{
  var zNumber = new ComplexNumberStruct(0.0f, 0.0f);
  var accumulator = 0;
  while (accumulator < iterations && zNumber.Magnitude() < 2.0)
  {
    zNumber = zNumber.MultiplyWith(zNumber).AddTo(number);
    ++accumulator;
  }
  return accumulator == iterations;
}

\end{lstlisting}

\clearpage
\section{NuGet package ranking}
NuGet is Microsoft-supported mechanism for sharing code, a package manager which defines how packages for .NET are created, hosted, and consumed, and provides the tools for each of those roles. It's a central package repository used by almost all .NET developers. Following software is used to download, analyze and rank NuGet packages by summing up the score of its dependencies and adding that to the score of the package itself. The software was implemented using MapReduce pattern (TODO: Add refrence).

Methods used in package ranking are defined in Lst. \ref{lst:NugetPackageRanking}. These functions will be later plugged into multiple, generic implementations of MapReduce pattern. \emph{Map} function is responsible for extracting key-value tuples from NuGet packages. Key represents the package name and float it's score, which will be later computed into the overall score of the package in the \emph{Reduce} function.

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Nuget package ranking functions}, label={lst:NugetPackageRanking}]
private static IEnumerable<(string, float score)> Map(NuGet.NuGetPackageCache package,
  ImmutableList<(string, float)> packages)
  => package.Dependencies.Select(x => (Domain.PackageName(x.Item1.Item1).Item1, GetScore(package, packages)));

private static float GetRanking(string packageName, ImmutableList<(string, float)> packages)
  => packages.Any(x => x.First() == packageName)
    ? packages.First(x => x.First() == packageName).Second()
    : 1.0f;

private static float GetScore(NuGet.NuGetPackageCache package, ImmutableList<(string, float)> packages)
  => GetRanking(package.PackageName, packages) / package.Dependencies.Length;

private static (string packageName, float) Reduce(string packageName, IEnumerable<float> values)
  => (packageName, values.Sum());
\end{lstlisting}

\subsubsection{Sequential implementation}
MapReduce pattern is described in detail in (TODO: Add reference). 
In simple terms \emph{Map} function is responsible for mapping objects into collections of key-value pairs. In this case it will transform \emph{NuGetPackage} objects into collection of packageName-score tuples. \empg{Reduce} function receives collection of values grouped by pairs, in this case groups of scores grouped by the name of the package. The pattern was implemented using LINQ, with eager materialization in both \emph{Map} and \emph{Reduce} to avoid multiple evaluation of the same sequence (Lst. \ref{lst:SeqMapReduce}).

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Sequential MapReduce implementation}, label={lst:SeqMapReduce}]
public static IEnumerable<TReduced> MapReduce<TIn, TOutKey, TOutValue, TReduced>(
  this IEnumerable<TIn> source,
  Func<TIn, IEnumerable<(TOutKey, TOutValue)>> map,
  Func<TOutKey, IEnumerable<TOutValue>, TReduced> reduce)
  => source
    .Map(map)
    .Reduce(reduce);

private static IEnumerable<IGrouping<TOutKey, (TOutKey, TOutValue)>> Map<TIn, TOutKey, TOutValue>(
  this IEnumerable<TIn> source,
  Func<TIn, IEnumerable<(TOutKey, TOutValue)>> map)
  => source
    .SelectMany(map)
    .GroupBy(Tuple.First)
    .ToImmutableList();

private static IEnumerable<TReduced> Reduce<TKey, TValue, TReduced>
(this IEnumerable<IGrouping<TKey, (TKey, TValue)>> source,
  Func<TKey, IEnumerable<TValue>, TReduced> reduce)
  => source
    .Select(x => reduce(x.Key, x.Select(Tuple.Second)))
    .ToImmutableList();

\end{lstlisting}

\subsubsection{Parallel implementation}
Parallel implementation adds additional paramater which controls the degree of parallelism to avoid oversaturation. This version was also implemented using PLINQ in a very intuitive and readable way, thanks to the use of MapReduce pattern (Lst. \ref{lst:ParMapReduce}).

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel MapReduce implementation}, label={lst:ParMapReduce}]
private static ParallelQuery<TSource> ToParallel<TSource>(this IEnumerable<TSource> source, int degreeOfParallelism)
  => source
    .AsParallel()
    .WithExecutionMode(ParallelExecutionMode.ForceParallelism)
    .WithDegreeOfParallelism(degreeOfParallelism);

private static IEnumerable<TReduced> MapReduce<TIn, TOutKey, TOutValue, TReduced>(
  this IEnumerable<TIn> source,
  Func<TIn, IEnumerable<(TOutKey, TOutValue)>> map,
  Func<TOutKey, IEnumerable<TOutValue>, TReduced> reduce, int mDOR, int rDOR)
  => source
    .ToParallel(mDOR)
    .Map(map)
    .ToParallel(rDOR)
    .Reduce(reduce)
    .ToImmutableList();
\end{lstlisting}

\subsubsection{Parallel implementation with partitioner}
In the last version load balancing partitioner was used to try to evenly spread the work between the threads (Lst. \ref{lst:PartParMapReduce}).

\begin{lstlisting}[language={[sharp]c}, style=sharpcstyle, caption={Parallel MapReduce implementation with partitioner}, label={lst:PartParMapReduce}]
private static ParallelQuery<TSource> ToParallel<TSource>(this OrderablePartitioner<TSource> source, int degreeOfParallelism)
  => source
    .AsParallel()
    .WithExecutionMode(ParallelExecutionMode.ForceParallelism)
    .WithDegreeOfParallelism(degreeOfParallelism);

private static IEnumerable<TReduced> PartitionerMapReduce<TIn, TOutKey, TOutValue, TReduced>(
  this IList<TIn> source,
  Func<TIn, IEnumerable<(TOutKey, TOutValue)>> map,
  Func<TOutKey, IEnumerable<TOutValue>, TReduced> reduce, int mDOR, int rDOR)
  => Partitioner
    .Create(source, true)
    .ToParallel(mDOR)
    .Map(map)
    .ToParallel(rDOR)
    .Reduce(reduce)
    .ToImmutableList();
\end{lstlisting}